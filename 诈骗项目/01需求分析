1、背景：
通过有计划的欺骗手段，大量骗取平台内其他用户的财产
例如即时通信平台中的交友诈骗、电商平台中的客服退款诈骗、在线会议平台中的仿冒公检法诈骗；冒充的居多；
2、诈骗整个产业链条：
一是高度产业化，黑产已经逐步形成了完整的产业化链条：上游提供各类技术支持，如验证码绕过、手机群控、自动注册工具等；
在中游，有人专门收集大量的手机号、身份证号、银行卡号等信息；
下游则是对中游资源的变现，如欺诈、恶意引流、刷单、“薅羊毛”等。
二是注重资源对抗，在黑产的中上游，由专门的团伙负责大批量收集各类资源，以供各种下游团伙使用，从而降低黑产攻击的成本。
在高度产业化和资源对抗这两个特点下，黑产具有丰富的类型。

3、诈骗用的技术工具
3-1 猫池

需要先介绍一下手机卡在其中发挥的作用。当前，用户在互联网平台进行账号注册时，往往需要通过手机号和验证码进行账号绑定。
因此，获取足量的可接收验证码的手机号是“号商”养号的基础。

3-2 接码平台
这些卡通过接码平台接码，提供给中下游的黑灰产从业者，用于网络刷量、网络诈骗等活动。
接码平台使用物联网卡或未经实名认证的手机卡来接收验证码
可以实现批量注册网络账号、绕开账号实名认证、绑定账号、解绑账号等操作，为各类网络犯罪活动提供了极大的便利
卡商”购买猫池及手机“黑卡”后，可以通过API等接口连接到接码平台上，在用手机号码注册电商平台或网站之后，
“卡商”就会接收到验证码短信，“卡商”将其打码后传送给接码平台，接码平台再传递给下游用户

3-3 打码平台
与验证码相关的黑产工具还有打码平台，打码平台可以通过自动或人工的方式识别验证码。
打码平台可以通过图像识别、语音识别等算法进行自动打码，打码平台会将验证码自动转为人工打码，
人工打码本质上是真人众包形式的网赚作恶项目，帮助黑产团伙绕过验证码

3-4 群控
以社交平台的群控养号为例，操控者可以通过群控去批量自动完善多个账号的信息
再通过一键添加好友、批量发送消息等功能提高账号活跃度和权重，达到养号的效果

3-5  改机工具
批量伪造新设备，
3-6 多开软件

3-7 虚拟定位



4、治理的方法论：压降和控制诈骗电话反弹趋势，需要

方法论：诈骗治理包含事前、事中、事后三个风控阶段。
在事前风控阶段，通过身份模型对用户、环境、设备判别，预防潜在风险；
在事中风控阶段，判断违规行为、恶意内容的安全风险，并进行阻断和拦截；
在事后风控阶段，对社群、产业、团伙进行全面复盘，挖掘潜在恶意同伙、产业链及组织分工，全面打击黑灰产业链


4-1  大数据平台——反欺诈体系
在大数据治理下的反欺诈体系建设中，对于数据从产生、采集、加工、存储、开发、应用到销毁的全过程
都需要引入高质量的平台和运营体系来保证大数据的安全

大数据平台
早期的分布式文件系统HDFS，分布式计算框架MapReduce到后续通用资源管理和调度系统Yarn
高效分布式协调服务ZooKeeper，分布式数据同步工具Sqoop，面向列的分布式KV存储系统HBase
实现数据表映射的数据仓库工具Hive，海量日志收集、聚合和传输系统Flume
高吞吐量的发布订阅消息系统Kafka，实时大数据处理框架Storm等多种中间件服务或框架
其间也诞生了基于内存计算方式的Spark框架


大数据平台的核心组成可分为三层：第一层就是底层数据存储，第二层是中间平台的计算，第三层是做数据分析的业务应用

大数据是由庞大的数据规模、快速的数据流转和多样的数据类型构成的


    4-1-1 计算框架
当下三个经典主流的大数据计算框架分别是Hadoop、Spark和Flink。
在这三个计算框架中，最早出现的Hadoop是由其创始人在MapReduce模型的启发下构建出来的。
Hadoop主要面向批处理任务，可以用来处理海量数据，目前已经成为许多企业主要的大数据解决方案。
而Spark具有比Hadoop更高的执行速度，通过提供许多具有易用性的接口，Spark在机器学习和图计算中被更广泛地应用。
与Spark在批处理领域的绝对优势不同，Flink在流处理领域一枝独秀，其性能也远超其他流处理的大数据计算框架


    4-1-2 存储框架
分布式存储  可以解决数据安全性和可靠性问题；同时利用多台存储服务器可以控制负载均衡

    4-1-3 计算模式
数据计算的方式主要可以分为批处理、流处理两种


4-2 大数据治理
大数据治理是对数字资产全生命周期进行管理。数据收集、清洗、存储、读取以及展示等过程。
大数据治理的流程主要可分为数据模型、元数据管理、数据质量管理、数据生命周期管理以及数据安全

    4-2-1 数据模型
三要素，分别为数据结构、数据操作和数据约束

    4-2-2 元数据管理
元数据是描述数据的数据，元数据是指数据系统所产生的描述、定义以及规则等数据
主要包含对数据的使用用途、结构信息、格式定义、存储方式等多个方面的说明

元数据管理可以保障数据质量。元数据管理包含三方面的工作
第一是创建元数据，需要抽象出数据的关键因素并将其用元数据进行描述；
第二是维护元数据，需要确认元数据的存储形态；
第三是建立元数据的模型（也就是元模型），用元模型来管理各个元数据

    4-2-3 数据质量管理
数据质量问题存在于从数据获取到数据消亡的整个生命周期

需要明确各个阶段的数据质量管理流程及数据质量的度量标准，
按照所定义的度量标准进行数据质量检测和规范，并及时进行数据质量治理，从而避免事后回溯，造成业务的损失

    4-2-4 数据生命周期管理
数据作为对事物客观规律的描述，在事物客观规律形成的初期，数据被采集并被用来表达这种规律
随着客观规律发生变化，数据也会逐渐失效
数据生命周期管理可以提高系统效率、大幅度减少数据存储成本，整个管理过程涵盖了数据的产生、加工、使用、失效以及淘汰
设置相应的存储时长、存储方式、存储规则和注意事项

    4-2-5 数据安全
第一层含义是在使用数据时数据是有效的，第二层含义是数据不会被非法利用

4-3 数据清洗
数据清洗的主要步骤包括缺失值处理、异常值处理以及归一化与标准化

    4-3-1 缺失值处理
    产生原因：采集端上报数据出错等机器因素，或者人为填报数据
    第一删除数据；第二填充数据

    4-3-2 异常值处理
    产生原因：采集端上报数据出错等机器因素，或者人为填报数据
    基础的异常值检测方法
    基于统计分布的异常值检测方法
            标准正态分布为例，数据约有99.7%的可能会落在距均值3个标准差的范围之内，那么与均值的差不在3个标准差范围内的数据可视为异常值
    基于聚类的异常值检测方法
            基于聚类的离群异常值检测方法会将异常数据视作离群点

    4-3-3 归一化与标准化
            数据的归一化和标准化都是将数据从原始的空间分布映射到另外一个更加有利于数据分析的分布
            归一化是将原始数据在同一量纲下压缩为0～1的小数
            标准化会使得数据本身的分布发生变化，例如通过z-score（z分数）的方法会使映射后的原始数据服从标准正态分布

            从训练模型的角度来看，模型就是通过自身的参数对输入特征数据做映射，使其尽量靠近样本标签
            训练模型就是不断迭代和优化参数，使其对标签的拟合效果更好

4-4 特征工程
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程就是基于相关知识将原始数据处理成特征的过程

    4-4-1 特征提取和构建
    基于基础统计学知识和业务经验的方式
    根据学历代码，学历一般分为小学、初中、高中、专科、本科、硕士和博士
    学历数据直接使用中文字符是不能被模型所识别的，需要将其构建为学历特征
    one-hot编码和简单的数字编码可以实现特征构建

    4-4-2 特征学习
        4-4-2-1 有监督学习
    有监督的特征学习是指在特征学习中引入样本信息，借助于样本信息，从原始数据中整合出有效的特征
    计算TGI(target group index)指数，是反映目标群体在特定研究范围内的强势或弱势的指数）指数来查看不同样本的偏好分布
    然后从原始数据中提取出TGI指数较高的基础属性来构建特征。

        4-4-2-2 无监督学习
    无监督的特征学习主要强调挖掘数据本身的规律
        时序规律：挖掘文本时序规律；word to vector；是一种通过训练浅层神经网络来学习文本表示的算法
                        采用的模型包含CBOW(Continuous Bag of Words)模型和Skip-gram模型
                        


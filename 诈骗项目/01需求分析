1、背景：
通过有计划的欺骗手段，大量骗取平台内其他用户的财产
例如即时通信平台中的交友诈骗、电商平台中的客服退款诈骗、在线会议平台中的仿冒公检法诈骗；冒充的居多；
2、诈骗整个产业链条：
一是高度产业化，黑产已经逐步形成了完整的产业化链条：上游提供各类技术支持，如验证码绕过、手机群控、自动注册工具等；
在中游，有人专门收集大量的手机号、身份证号、银行卡号等信息；
下游则是对中游资源的变现，如欺诈、恶意引流、刷单、“薅羊毛”等。
二是注重资源对抗，在黑产的中上游，由专门的团伙负责大批量收集各类资源，以供各种下游团伙使用，从而降低黑产攻击的成本。
在高度产业化和资源对抗这两个特点下，黑产具有丰富的类型。

3、诈骗用的技术工具
3-1 猫池

需要先介绍一下手机卡在其中发挥的作用。当前，用户在互联网平台进行账号注册时，往往需要通过手机号和验证码进行账号绑定。
因此，获取足量的可接收验证码的手机号是“号商”养号的基础。

3-2 接码平台
这些卡通过接码平台接码，提供给中下游的黑灰产从业者，用于网络刷量、网络诈骗等活动。
接码平台使用物联网卡或未经实名认证的手机卡来接收验证码
可以实现批量注册网络账号、绕开账号实名认证、绑定账号、解绑账号等操作，为各类网络犯罪活动提供了极大的便利
卡商”购买猫池及手机“黑卡”后，可以通过API等接口连接到接码平台上，在用手机号码注册电商平台或网站之后，
“卡商”就会接收到验证码短信，“卡商”将其打码后传送给接码平台，接码平台再传递给下游用户

3-3 打码平台
与验证码相关的黑产工具还有打码平台，打码平台可以通过自动或人工的方式识别验证码。
打码平台可以通过图像识别、语音识别等算法进行自动打码，打码平台会将验证码自动转为人工打码，
人工打码本质上是真人众包形式的网赚作恶项目，帮助黑产团伙绕过验证码

3-4 群控
以社交平台的群控养号为例，操控者可以通过群控去批量自动完善多个账号的信息
再通过一键添加好友、批量发送消息等功能提高账号活跃度和权重，达到养号的效果

3-5  改机工具
批量伪造新设备，
3-6 多开软件

3-7 虚拟定位



4、治理的方法论：压降和控制诈骗电话反弹趋势，需要

方法论：诈骗治理包含事前、事中、事后三个风控阶段。
在事前风控阶段，通过身份模型对用户、环境、设备判别，预防潜在风险；
在事中风控阶段，判断违规行为、恶意内容的安全风险，并进行阻断和拦截；
在事后风控阶段，对社群、产业、团伙进行全面复盘，挖掘潜在恶意同伙、产业链及组织分工，全面打击黑灰产业链


4-1  大数据平台——反欺诈体系
在大数据治理下的反欺诈体系建设中，对于数据从产生、采集、加工、存储、开发、应用到销毁的全过程
都需要引入高质量的平台和运营体系来保证大数据的安全

大数据平台
早期的分布式文件系统HDFS，分布式计算框架MapReduce到后续通用资源管理和调度系统Yarn
高效分布式协调服务ZooKeeper，分布式数据同步工具Sqoop，面向列的分布式KV存储系统HBase
实现数据表映射的数据仓库工具Hive，海量日志收集、聚合和传输系统Flume
高吞吐量的发布订阅消息系统Kafka，实时大数据处理框架Storm等多种中间件服务或框架
其间也诞生了基于内存计算方式的Spark框架


大数据平台的核心组成可分为三层：第一层就是底层数据存储，第二层是中间平台的计算，第三层是做数据分析的业务应用

大数据是由庞大的数据规模、快速的数据流转和多样的数据类型构成的


    4-1-1 计算框架
当下三个经典主流的大数据计算框架分别是Hadoop、Spark和Flink。
在这三个计算框架中，最早出现的Hadoop是由其创始人在MapReduce模型的启发下构建出来的。
Hadoop主要面向批处理任务，可以用来处理海量数据，目前已经成为许多企业主要的大数据解决方案。
而Spark具有比Hadoop更高的执行速度，通过提供许多具有易用性的接口，Spark在机器学习和图计算中被更广泛地应用。
与Spark在批处理领域的绝对优势不同，Flink在流处理领域一枝独秀，其性能也远超其他流处理的大数据计算框架


    4-1-2 存储框架
分布式存储  可以解决数据安全性和可靠性问题；同时利用多台存储服务器可以控制负载均衡

    4-1-3 计算模式
数据计算的方式主要可以分为批处理、流处理两种


4-2 大数据治理
大数据治理是对数字资产全生命周期进行管理。数据收集、清洗、存储、读取以及展示等过程。
大数据治理的流程主要可分为数据模型、元数据管理、数据质量管理、数据生命周期管理以及数据安全

    4-2-1 数据模型
三要素，分别为数据结构、数据操作和数据约束

    4-2-2 元数据管理
元数据是描述数据的数据，元数据是指数据系统所产生的描述、定义以及规则等数据
主要包含对数据的使用用途、结构信息、格式定义、存储方式等多个方面的说明

元数据管理可以保障数据质量。元数据管理包含三方面的工作
第一是创建元数据，需要抽象出数据的关键因素并将其用元数据进行描述；
第二是维护元数据，需要确认元数据的存储形态；
第三是建立元数据的模型（也就是元模型），用元模型来管理各个元数据

    4-2-3 数据质量管理
数据质量问题存在于从数据获取到数据消亡的整个生命周期

需要明确各个阶段的数据质量管理流程及数据质量的度量标准，
按照所定义的度量标准进行数据质量检测和规范，并及时进行数据质量治理，从而避免事后回溯，造成业务的损失

    4-2-4 数据生命周期管理
数据作为对事物客观规律的描述，在事物客观规律形成的初期，数据被采集并被用来表达这种规律
随着客观规律发生变化，数据也会逐渐失效
数据生命周期管理可以提高系统效率、大幅度减少数据存储成本，整个管理过程涵盖了数据的产生、加工、使用、失效以及淘汰
设置相应的存储时长、存储方式、存储规则和注意事项

    4-2-5 数据安全
第一层含义是在使用数据时数据是有效的，第二层含义是数据不会被非法利用

4-3 数据清洗
数据清洗的主要步骤包括缺失值处理、异常值处理以及归一化与标准化

    4-3-1 缺失值处理
    产生原因：采集端上报数据出错等机器因素，或者人为填报数据
    第一删除数据；第二填充数据

    4-3-2 异常值处理
    产生原因：采集端上报数据出错等机器因素，或者人为填报数据
    基础的异常值检测方法
    基于统计分布的异常值检测方法
            标准正态分布为例，数据约有99.7%的可能会落在距均值3个标准差的范围之内，那么与均值的差不在3个标准差范围内的数据可视为异常值
    基于聚类的异常值检测方法
            基于聚类的离群异常值检测方法会将异常数据视作离群点

    4-3-3 归一化与标准化
            数据的归一化和标准化都是将数据从原始的空间分布映射到另外一个更加有利于数据分析的分布
            归一化是将原始数据在同一量纲下压缩为0～1的小数
            标准化会使得数据本身的分布发生变化，例如通过z-score（z分数）的方法会使映射后的原始数据服从标准正态分布

            从训练模型的角度来看，模型就是通过自身的参数对输入特征数据做映射，使其尽量靠近样本标签
            训练模型就是不断迭代和优化参数，使其对标签的拟合效果更好

4-4 特征工程
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程就是基于相关知识将原始数据处理成特征的过程

    4-4-1 特征提取和构建
    基于基础统计学知识和业务经验的方式
    根据学历代码，学历一般分为小学、初中、高中、专科、本科、硕士和博士
    学历数据直接使用中文字符是不能被模型所识别的，需要将其构建为学历特征
    one-hot编码和简单的数字编码可以实现特征构建

    4-4-2 特征学习
        4-4-2-1 有监督学习
    有监督的特征学习是指在特征学习中引入样本信息，借助于样本信息，从原始数据中整合出有效的特征
    计算TGI(target group index)指数，是反映目标群体在特定研究范围内的强势或弱势的指数）指数来查看不同样本的偏好分布
    然后从原始数据中提取出TGI指数较高的基础属性来构建特征。

        4-4-2-2 无监督学习
    无监督的特征学习主要强调挖掘数据本身的规律
        时序规律：挖掘文本时序规律；word to vector；是一种通过训练浅层神经网络来学习文本表示的算法
                        采用的模型包含CBOW(Continuous Bag of Words)模型和Skip-gram模型


                         
                        AutoEncoder算法，AutoEncoder算法的架构是一种特殊的神经网络架构，其中输入和输出的组成是相似的
                            分别包括了编码器和解码器首先我们将HTML文本中的URL关键词转换为one-hot编码
                            然后将one-hot编码输入到AutoEncoder算法的网络架构中，通过编码器将其降维成低维空间数据，也就是Embedding向量
                                    备注：Embedding 就是用一个数值向量“表示”一个对象（Object）的方法
                            在AutoEncoder算法中，如果使用解码器将Embedding向量解码为重构数据，重构数据与输入是相似的
                            那么就可以认为这个编码到低维空间的Embedding向量几乎没有信息损失，也就是说这个Embedding向量对这个URL关键词的信息表达是充分的

    4-4-3 特征评估与选择

            为什么要进行选择：在使用多种方式构建了大量的特征后，直接通过这些特征去训练模型是有问题的
                                            一方面训练开销会比较大，另一方面特征太多会产生模型训练的收敛速度慢等问题       

            过滤法filter：特征与结果的相关性；过滤法首先需要去评估每个特征，然后量化每个特征的有效性，再通过量化的值对特征进行排序，最终按照需要截取排序靠前的特征进行建模。
                    过滤法的评估复杂度较低，但是不会考虑特征之间的叠加作用
                    分箱计算证据权重(weight of evidence)值：以用户在交友平台填写的年龄资料为例，以5年为一个区间分段，取25～30岁的用户区间进行计算。
                    在该区间中可疑用户与正常用户的比率为3.27%，在整体用户中可疑用户与正常用户的比率为0.34%
                    通过计算得到该区间的WOE值为2.26，表明这一年龄取值区间对于是否为诈骗账号具有预测能力

            包装法wrapper：挑选特征去训练，通过模型的评估性能指标来选择；
                    递归特征删除算法：基模型训练，将权重较低的特征进行删除，选出一个最优的特征子集

            嵌入法：将特征选择与模型训练结合一体
                    基于树模型的特征选择，GBDT

5、大数据安全对抗技术  与反欺诈实战案例
5-1 人机验证
        图灵测试是用来测试机器是否具备像人类一样思考的能力
        逆图灵测试  设计算法来验证互联网的访问者是真人还是机器

        人机验证程序：防御黑产的批量机器人攻击，人机验证只能对明显异常的偏机器人或者自动脚本的流量进行初筛
    5-1-1 字符验证码
            因为不管业务方如何升级字符验证码，黑产都能通过反复试探和摸索对抗方式攻破字符验证码
    5-1-2 行为验证码
            光学字符识别技术(OCR)逐渐成为黑产对抗字符验证码的利器；
            行为验证码摒弃了多年来对字符的依赖，采用图像作为验证码载体，为验证码的构建提供了更多可发挥的空间
            即————滑块拼图验证码，
                    但是黑产基于目标检测和模拟人类习惯的滑动轨迹，最终攻破了这种验证方式

            随后————点选图形验证码，点选图形验证码增加了文字区分的功能和对点击顺序的要求，所以安全性大幅提升

            再后————新型验证码，比较新颖的验证码有智能推理验证码、无感验证码
                   主要融入了人类的逻辑推理以及多维空间的元素辨别能力，智能推理验证码适用于银行等高安全需求的流量威胁场景

            再后————无感验证码，是基于用户行为信息、环境信息以及设备指纹等多维度信息，综合进行智能人机识别的新型验证方式
                    无感验证码可以根据用户的风险程度，自动弹出不同难度的二次验证方式，正常用户只需轻点即可通过验证。
                    该类型验证码的优点是安全性高且用户无感知
5-2 风险名单
    5-2-1
        风险名单主要应用在流量威胁的前置风险筛查环节
        黑产资源具备有限性，黑产会在多个业务平台反复使用自己掌控的黑产资源，直到被大部分平台封禁为止。
    5-2-2 风险名单设计
            从风险等级角度，风险名单可以分为黑名单、灰名单和白名单；
            从业务场景角度，风险名单可以分为不区分业务场景的通用风险名单和区分具体业务场景的业务风险名单
            业务场景角度设计
                    通用黑名单：
                    业务经验名单：
    5-2-3 风险名单的管理
            根据具体业务的外网处罚和投诉情况，进行线上实时效果监控
            风险名单具有时效性，如果不及时对旧的名单数据进行淘汰，可能导致线上误处罚
                具体可以从两个方面进行设置：一方面设置固定时间窗口，主动淘汰旧的名单数据；
                另一方面通过监控线上实时投诉率，及时淘汰旧的名单数据。

5-3 规则引擎
        风险名单只能拦截历史上有被判黑过的欺诈账户，而对于未在风险名单中的欺诈账号，需通过专家规则来进一步识别
        专家规则可以根据业务通用性分为基础通用规则和业务定制规则
    5-3-1 基础通用规则
        而批量操作的前提是绕过业务方的频控，例如同一个IP/设备/账号在固定时间内的访问次数不能超过一定次数。
            基础通用规则主要从三方面构建，即IP、设备和账号

    5-3-2 

    



        



            

            


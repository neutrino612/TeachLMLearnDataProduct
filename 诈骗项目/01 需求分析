1、背景：
通过有计划的欺骗手段，大量骗取平台内其他用户的财产
例如即时通信平台中的交友诈骗、电商平台中的客服退款诈骗、在线会议平台中的仿冒公检法诈骗；冒充的居多；
2、诈骗整个产业链条：
一是高度产业化，黑产已经逐步形成了完整的产业化链条：上游提供各类技术支持，如验证码绕过、手机群控、自动注册工具等；
在中游，有人专门收集大量的手机号、身份证号、银行卡号等信息；
下游则是对中游资源的变现，如欺诈、恶意引流、刷单、“薅羊毛”等。
二是注重资源对抗，在黑产的中上游，由专门的团伙负责大批量收集各类资源，以供各种下游团伙使用，从而降低黑产攻击的成本。
在高度产业化和资源对抗这两个特点下，黑产具有丰富的类型。

3、诈骗用的技术工具
3-1 猫池

需要先介绍一下手机卡在其中发挥的作用。当前，用户在互联网平台进行账号注册时，往往需要通过手机号和验证码进行账号绑定。
因此，获取足量的可接收验证码的手机号是“号商”养号的基础。

3-2 接码平台
这些卡通过接码平台接码，提供给中下游的黑灰产从业者，用于网络刷量、网络诈骗等活动。
接码平台使用物联网卡或未经实名认证的手机卡来接收验证码
可以实现批量注册网络账号、绕开账号实名认证、绑定账号、解绑账号等操作，为各类网络犯罪活动提供了极大的便利
卡商”购买猫池及手机“黑卡”后，可以通过API等接口连接到接码平台上，在用手机号码注册电商平台或网站之后，
“卡商”就会接收到验证码短信，“卡商”将其打码后传送给接码平台，接码平台再传递给下游用户

3-3 打码平台
与验证码相关的黑产工具还有打码平台，打码平台可以通过自动或人工的方式识别验证码。
打码平台可以通过图像识别、语音识别等算法进行自动打码，打码平台会将验证码自动转为人工打码，
人工打码本质上是真人众包形式的网赚作恶项目，帮助黑产团伙绕过验证码

3-4 群控
以社交平台的群控养号为例，操控者可以通过群控去批量自动完善多个账号的信息
再通过一键添加好友、批量发送消息等功能提高账号活跃度和权重，达到养号的效果

3-5  改机工具
批量伪造新设备，
3-6 多开软件

3-7 虚拟定位



4、治理的方法论：压降和控制诈骗电话反弹趋势，需要

方法论：诈骗治理包含事前、事中、事后三个风控阶段。
在事前风控阶段，通过身份模型对用户、环境、设备判别，预防潜在风险；
在事中风控阶段，判断违规行为、恶意内容的安全风险，并进行阻断和拦截；
在事后风控阶段，对社群、产业、团伙进行全面复盘，挖掘潜在恶意同伙、产业链及组织分工，全面打击黑灰产业链


4-1  大数据平台——反欺诈体系
在大数据治理下的反欺诈体系建设中，对于数据从产生、采集、加工、存储、开发、应用到销毁的全过程
都需要引入高质量的平台和运营体系来保证大数据的安全

大数据平台
早期的分布式文件系统HDFS，分布式计算框架MapReduce到后续通用资源管理和调度系统Yarn
高效分布式协调服务ZooKeeper，分布式数据同步工具Sqoop，面向列的分布式KV存储系统HBase
实现数据表映射的数据仓库工具Hive，海量日志收集、聚合和传输系统Flume
高吞吐量的发布订阅消息系统Kafka，实时大数据处理框架Storm等多种中间件服务或框架
其间也诞生了基于内存计算方式的Spark框架


大数据平台的核心组成可分为三层：第一层就是底层数据存储，第二层是中间平台的计算，第三层是做数据分析的业务应用

大数据是由庞大的数据规模、快速的数据流转和多样的数据类型构成的


    4-1-1 计算框架
当下三个经典主流的大数据计算框架分别是Hadoop、Spark和Flink。
在这三个计算框架中，最早出现的Hadoop是由其创始人在MapReduce模型的启发下构建出来的。
Hadoop主要面向批处理任务，可以用来处理海量数据，目前已经成为许多企业主要的大数据解决方案。
而Spark具有比Hadoop更高的执行速度，通过提供许多具有易用性的接口，Spark在机器学习和图计算中被更广泛地应用。
与Spark在批处理领域的绝对优势不同，Flink在流处理领域一枝独秀，其性能也远超其他流处理的大数据计算框架


    4-1-2 存储框架
分布式存储  可以解决数据安全性和可靠性问题；同时利用多台存储服务器可以控制负载均衡

    4-1-3 计算模式
数据计算的方式主要可以分为批处理、流处理两种


4-2 大数据治理
大数据治理是对数字资产全生命周期进行管理。数据收集、清洗、存储、读取以及展示等过程。
大数据治理的流程主要可分为数据模型、元数据管理、数据质量管理、数据生命周期管理以及数据安全

    4-2-1 数据模型
三要素，分别为数据结构、数据操作和数据约束

    4-2-2 元数据管理
元数据是描述数据的数据，元数据是指数据系统所产生的描述、定义以及规则等数据
主要包含对数据的使用用途、结构信息、格式定义、存储方式等多个方面的说明

元数据管理可以保障数据质量。元数据管理包含三方面的工作
第一是创建元数据，需要抽象出数据的关键因素并将其用元数据进行描述；
第二是维护元数据，需要确认元数据的存储形态；
第三是建立元数据的模型（也就是元模型），用元模型来管理各个元数据

    4-2-3 数据质量管理
数据质量问题存在于从数据获取到数据消亡的整个生命周期

需要明确各个阶段的数据质量管理流程及数据质量的度量标准，
按照所定义的度量标准进行数据质量检测和规范，并及时进行数据质量治理，从而避免事后回溯，造成业务的损失

    4-2-4 数据生命周期管理
数据作为对事物客观规律的描述，在事物客观规律形成的初期，数据被采集并被用来表达这种规律
随着客观规律发生变化，数据也会逐渐失效
数据生命周期管理可以提高系统效率、大幅度减少数据存储成本，整个管理过程涵盖了数据的产生、加工、使用、失效以及淘汰
设置相应的存储时长、存储方式、存储规则和注意事项

    4-2-5 数据安全
第一层含义是在使用数据时数据是有效的，第二层含义是数据不会被非法利用

4-3 数据清洗
数据清洗的主要步骤包括缺失值处理、异常值处理以及归一化与标准化

    4-3-1 缺失值处理
    产生原因：采集端上报数据出错等机器因素，或者人为填报数据
    第一删除数据；第二填充数据

    4-3-2 异常值处理
    产生原因：采集端上报数据出错等机器因素，或者人为填报数据
    基础的异常值检测方法
    基于统计分布的异常值检测方法
            标准正态分布为例，数据约有99.7%的可能会落在距均值3个标准差的范围之内，那么与均值的差不在3个标准差范围内的数据可视为异常值
    基于聚类的异常值检测方法
            基于聚类的离群异常值检测方法会将异常数据视作离群点

    4-3-3 归一化与标准化
            数据的归一化和标准化都是将数据从原始的空间分布映射到另外一个更加有利于数据分析的分布
            归一化是将原始数据在同一量纲下压缩为0～1的小数
            标准化会使得数据本身的分布发生变化，例如通过z-score（z分数）的方法会使映射后的原始数据服从标准正态分布

            从训练模型的角度来看，模型就是通过自身的参数对输入特征数据做映射，使其尽量靠近样本标签
            训练模型就是不断迭代和优化参数，使其对标签的拟合效果更好

4-4 特征工程
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程就是基于相关知识将原始数据处理成特征的过程

    4-4-1 特征提取和构建
    基于基础统计学知识和业务经验的方式
    根据学历代码，学历一般分为小学、初中、高中、专科、本科、硕士和博士
    学历数据直接使用中文字符是不能被模型所识别的，需要将其构建为学历特征
    one-hot编码和简单的数字编码可以实现特征构建

    4-4-2 特征学习
        4-4-2-1 有监督学习
    有监督的特征学习是指在特征学习中引入样本信息，借助于样本信息，从原始数据中整合出有效的特征
    计算TGI(target group index)指数，是反映目标群体在特定研究范围内的强势或弱势的指数）指数来查看不同样本的偏好分布
    然后从原始数据中提取出TGI指数较高的基础属性来构建特征。

        4-4-2-2 无监督学习
    无监督的特征学习主要强调挖掘数据本身的规律
        时序规律：挖掘文本时序规律；word to vector；是一种通过训练浅层神经网络来学习文本表示的算法
                        采用的模型包含CBOW(Continuous Bag of Words)模型和Skip-gram模型


                         
                        AutoEncoder算法，AutoEncoder算法的架构是一种特殊的神经网络架构，其中输入和输出的组成是相似的
                            分别包括了编码器和解码器首先我们将HTML文本中的URL关键词转换为one-hot编码
                            然后将one-hot编码输入到AutoEncoder算法的网络架构中，通过编码器将其降维成低维空间数据，也就是Embedding向量
                                    备注：Embedding 就是用一个数值向量“表示”一个对象（Object）的方法
                            在AutoEncoder算法中，如果使用解码器将Embedding向量解码为重构数据，重构数据与输入是相似的
                            那么就可以认为这个编码到低维空间的Embedding向量几乎没有信息损失，也就是说这个Embedding向量对这个URL关键词的信息表达是充分的

    4-4-3 特征评估与选择

            为什么要进行选择：在使用多种方式构建了大量的特征后，直接通过这些特征去训练模型是有问题的
                                            一方面训练开销会比较大，另一方面特征太多会产生模型训练的收敛速度慢等问题       

            过滤法filter：特征与结果的相关性；过滤法首先需要去评估每个特征，然后量化每个特征的有效性，再通过量化的值对特征进行排序，最终按照需要截取排序靠前的特征进行建模。
                    过滤法的评估复杂度较低，但是不会考虑特征之间的叠加作用
                    分箱计算证据权重(weight of evidence)值：以用户在交友平台填写的年龄资料为例，以5年为一个区间分段，取25～30岁的用户区间进行计算。
                    在该区间中可疑用户与正常用户的比率为3.27%，在整体用户中可疑用户与正常用户的比率为0.34%
                    通过计算得到该区间的WOE值为2.26，表明这一年龄取值区间对于是否为诈骗账号具有预测能力

            包装法wrapper：挑选特征去训练，通过模型的评估性能指标来选择；
                    递归特征删除算法：基模型训练，将权重较低的特征进行删除，选出一个最优的特征子集

            嵌入法：将特征选择与模型训练结合一体
                    基于树模型的特征选择，GBDT

5、大数据安全对抗技术  与反欺诈实战案例
5-1 人机验证
        图灵测试是用来测试机器是否具备像人类一样思考的能力
        逆图灵测试  设计算法来验证互联网的访问者是真人还是机器

        人机验证程序：防御黑产的批量机器人攻击，人机验证只能对明显异常的偏机器人或者自动脚本的流量进行初筛
    5-1-1 字符验证码
            因为不管业务方如何升级字符验证码，黑产都能通过反复试探和摸索对抗方式攻破字符验证码
    5-1-2 行为验证码
            光学字符识别技术(OCR)逐渐成为黑产对抗字符验证码的利器；
            行为验证码摒弃了多年来对字符的依赖，采用图像作为验证码载体，为验证码的构建提供了更多可发挥的空间
            即————滑块拼图验证码，
                    但是黑产基于目标检测和模拟人类习惯的滑动轨迹，最终攻破了这种验证方式

            随后————点选图形验证码，点选图形验证码增加了文字区分的功能和对点击顺序的要求，所以安全性大幅提升

            再后————新型验证码，比较新颖的验证码有智能推理验证码、无感验证码
                   主要融入了人类的逻辑推理以及多维空间的元素辨别能力，智能推理验证码适用于银行等高安全需求的流量威胁场景

            再后————无感验证码，是基于用户行为信息、环境信息以及设备指纹等多维度信息，综合进行智能人机识别的新型验证方式
                    无感验证码可以根据用户的风险程度，自动弹出不同难度的二次验证方式，正常用户只需轻点即可通过验证。
                    该类型验证码的优点是安全性高且用户无感知
5-2 风险名单
    5-2-1
        风险名单主要应用在流量威胁的前置风险筛查环节
        黑产资源具备有限性，黑产会在多个业务平台反复使用自己掌控的黑产资源，直到被大部分平台封禁为止。
    5-2-2 风险名单设计
            从风险等级角度，风险名单可以分为黑名单、灰名单和白名单；
            从业务场景角度，风险名单可以分为不区分业务场景的通用风险名单和区分具体业务场景的业务风险名单
            业务场景角度设计
                    通用黑名单：
                    业务经验名单：
    5-2-3 风险名单的管理
            根据具体业务的外网处罚和投诉情况，进行线上实时效果监控
            风险名单具有时效性，如果不及时对旧的名单数据进行淘汰，可能导致线上误处罚
                具体可以从两个方面进行设置：一方面设置固定时间窗口，主动淘汰旧的名单数据；
                另一方面通过监控线上实时投诉率，及时淘汰旧的名单数据。

5-3 规则引擎
        风险名单只能拦截历史上有被判黑过的欺诈账户，而对于未在风险名单中的欺诈账号，需通过专家规则来进一步识别
        专家规则可以根据业务通用性分为基础通用规则和业务定制规则
    5-3-1 基础通用规则
        而批量操作的前提是绕过业务方的频控，例如同一个IP/设备/账号在固定时间内的访问次数不能超过一定次数。
            基础通用规则主要从三方面构建，即IP、设备和账号
            5-3-1-1 IP
            （1）风险IP识别和对抗策略：
                    IP频控策略：业务方的防御比较简单，限制IP访问的频率即可，如同一个IP在固定时间内的访问次数不能超过一定次数
            （2）代理IP识别：
                    代理IP黑库
            （3）秒拨IP：主要是利用家用宽带拨号上网的原理，每次断电重启即可获取新的IP
            5-3-1-2 设备
            （1）模拟器  假机假用户    主要通过提取模拟器软件的底层特征信息与真机对比，从特征层识别模拟器类型。
                                比如提取机型、CPU等底层特征信息与真机进行对比识别。
            （2）群控      真机假用户   业务方可以基于群控软件、廉价旧机型、手机电池状态等特征来综合识别设备农场这类真机假用户行为。
            （3）众包设备   真机真用户  众包模式的识别难度较大，目前主要通过检测众包相关的软件使用行为来识别

            5-3-1-3 账号
            （1）基于虚拟运营商卡的异常识别：  黑库规则
            （2）基于物联网卡的异常识别：
                    物联网卡是由三大运营商（移动、联通、电信）提供的、基于物联网专网的纯流量卡
                     物联网卡用来满足智能硬件的联网、管理以及集团公司的移动信息化应用需求，主要是以146、148等号段开头的卡。
                     大部分平台业务主要通过号段来识别，从而直接限制这类卡注册

            （3）基于海外卡的异常识别：
                    通过累积的黑库和海外地理位置的风险程度来综合识别
            （4）基于接码平台卡的异常识别：
                    业务方可以监控和收集接码平台的黑号，并搭建出黑号库
        
    5-3-2   业务定制规则
                业务规则自动生成系统：
                总共分成5个模块：输入自动预处理模块、规则自动生成模块、规则自动评估模块、规则自动上线模块和规则线上实时监控模块
            
            5-3-2-1 基于1-gram进行规则维度初筛
                基于输入的标准化格式字段，通过计算单一维度聚集群体的属性值来判断该维度的风险程度
                示例：计算群体的业务历史黑名单比例、白名单比例、投诉比例等属性值

            5-3-2-2 基于n-gram规则进行自动组合
                根据第一步初筛后的维度，基于n-gram进行维度组合，形成n-gram的规则key，然后通过这些规则key聚集用户群体
                示例：

            5-3-2-3 规则自动评估模块
                基于上述自动生成的规则key，进一步计算聚集到的用户群体的属性值（业务历史黑名单比例、白名单比例、投诉比例等）
            5-3-2-4 规则自动上线模块
                基于业务线上系统，通过灰度方式，将自动评估模块筛选出来的恶意规则池中的规则key上线，并进行实时处罚或监控
            5-3-2-4 规则线上实时监控模块
                主要从两方面进行，一是设置规则过期的时间窗口，进行主动淘汰；
                二是构建线上实时监控，对每个处罚规则key的投诉比例进行实时统计，一旦触发投诉比例阈值，就让规则key的处罚失效
                然后从恶意规则池中剔除规则key。
5-4 异常检测模型
            基于规则的范式可以对于检测已出现的流量欺诈模式有不错的方式，但是检测变异后或者新出现的欺诈模式比较捉襟见肘
            利用机器学习模型的泛化，可以主动发现变异和新出现的欺诈模式
            无监督模型  有监督模型
    5-4-1 传统统计检验
            互联网的流量大数据主要是基于广大用户的访问习惯形成的，所以从整体上来看，正常流量一定是遵循特定分布形式的
            而异常流量往往存在背离常理的分布

            5-4-1-1 基于3 Sigma准则识别异常流量
                假设某互联网业务场景的访问流量（在访问时间维度）服从正态分布，但是该方法是以假定业务场景流量服从正态分布为前提的

            5-4-1-2 基于Tukey箱型图法识别异常流量
                Tukey箱型图法对于业务场景流量的分布没有特殊要求，它主要是基于四分位距(IQR)的思想来构建箱型图

    5-4-2 无监督学习
                传统统计检验方法主要从单维度对业务流量进行异常检测，而实际中的业务流量数据往往是多维的。
            5-4-2-1 基于距离度量的异常检测模型
                业务流量数据多维空间，异常流量数据点往往以离群点的方式出现；正常流量数据点以簇的方式高度聚集
                KNN是基于距离度量的异常检测模型的代表算法
            5-4-2-2 基于密度的异常检测模型
                在业务流量数据的多维空间，正常流量数据点出现的区域密度高，而异常流量数据点出现的区域密度低
                具体表现为稀疏甚至是单个离群点的形式。
            5-4-2-3 基于降维思想的异常检测模型
                高维特征空间的处理开销很大，尤其是处理海量的业务流量数据时，不得不考虑计算开销。
                PCA异常检测模型，PCA在做特征值分解之后得到的特征向量反映了原始数据方差变化程度的不同方向
                特征值为数据在对应方向上的方差大小

            5-4-2-4 基于集成学习思想的异常检测模型
                孤立森林。
                对于正常流量数据点，由于是高度聚集的密集区域，因此需要被切割很多次才可以将每个流量数据点划分开；
                而异常流量数据点处于稀疏区域，每个数据点很容易被划分开

    5-4-3 半监督模型——异常检测模型
                当有样本时，我们又可以进一步升级模型。对于流量异常检测场景，异常样本比较难被获取到。
                异常样本的数量相对正常白样本的数量更少，此时训练二分类模型比较困难
            5-4-3-1 单分类SVM模型
                在正常流量与异常流量间寻找一个超平面，可以把正常流量和异常流量分开。
                单分类SVM模型的优点是不需要异常样本即可训练模型，适用于高维业务流量场景。
                该模型的缺点是计算核函数时速度慢，不太适合海量业务流量场景。

            5-4-3-2 AutoEncoder模型
                对于正常业务流量样本可以正常重构还原，而异常业务流量样本在重构过程中误差较大，无法较好地还原
                从而可以作为异常样本识别出来。

5-5 多模态集成模型
                上述介绍的机器模型都属于单模态检测，对黑产的覆盖能力有限。
                多模态集成模型可以利用多模态数据之间的信息互补关系，提升模型的泛化性，并进一步提升对黑产的覆盖能力

    5-5-1 多模态子模型
            5-5-1-1 关系图谱子模型


            5-5-1-2 文本子模型
            基于活动过程产生的垃圾评论、黄赌等引流文本或欺诈类文本，可以利用fastText、TextCNN、LSTM、BERT等自然语言处理算法


            5-5-1-3 图像子模型
            5-5-1-4 其他模态子模型
            视频、语音形态数据
    5.5.2 多模态集成模型

5.6 新型对抗方案

    5.6.1 小样本场景问题
            迁移学习来解决流量风控中的小样本场景问题
    5.6.2 跨平台联防联控问题
            如果各平台只关注这些黑号在自己业务上的历史行为表现，就很难提前感知到黑号的风险，
            而跨平台联防联控方式有助于业务侧更早地感知和管控黑产流量

            用户是原始数据的拥有者，在没有用户授权的情况下，公司或者平台间严禁交换数据。
            因此，数据合规性会导致即使在同一个公司内的不同业务平台中，数据往往以“孤岛”形式出现。

            联邦学习的诞生，正是为了解决各平台面临的“数据孤岛”和数据隐私的问题。
            联邦学习是一种机器学习框架，能让两方或多方数据不出本地也能被共同使用和建模
            横向联邦学习
            纵向联邦学习
            联邦迁移学习

6. 基于内容的对抗技术基于内容的对抗技术
    通过自然语言处理、计算机视觉以及多模态技术，来完成内容安全的反欺诈体系建设
    从而对大规模的内容数据中的恶意信息进行检测和识别，保障大数据时代背景下的内容安全

    6.1 业务场景与风险
        从不同场景出发，来分析对应内容的风险特性及应对思路，从而构建出内容安全整体框架
        

            






    



        



            

            

